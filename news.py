import time
from GoogleNews import GoogleNews
import datetime
import urllib.parse

def clean_link(link):
    """ ä¿®æ­£éŒ¯èª¤é€£çµï¼Œåˆªé™¤ Google è¿½è¹¤åƒæ•¸ """
    if "&ved=" in link:
        link = link.split("&ved=")[0]  # åˆªé™¤ `&ved=` ä¹‹å¾Œçš„å…§å®¹
    if "&usg=" in link:
        link = link.split("&usg=")[0]  # åˆªé™¤ `&usg=` ä¹‹å¾Œçš„å…§å®¹
    link = urllib.parse.unquote(link).strip()  # è§£ç¢¼ URLï¼Œç§»é™¤ç©ºæ ¼
    return link

def fetch_news():
    """ çˆ¬å–æœ€è¿‘ 7 å¤©çš„å°ç©é›»æ–°èï¼ˆæœ€å¤š 35 å‰‡ï¼‰ """
    query = "å°ç©é›»"
    googlenews = GoogleNews(lang="zh-TW", region="TW")

    # è¨ˆç®—æœ€è¿‘ 7 å¤©çš„æ—¥æœŸç¯„åœ
    today = datetime.datetime.today()
    seven_days_ago = today - datetime.timedelta(days=7)
    from_date = seven_days_ago.strftime("%m/%d/%Y")
    to_date = today.strftime("%m/%d/%Y")

    # è¨­å®šæ–°èæ™‚é–“ç¯„åœä¸¦æœå°‹
    googlenews.clear()
    googlenews.set_time_range(from_date, to_date)

    # å–å¾—æ–°è
    news_results = []
    seen_titles = set()  # ğŸ”¹ è¨˜éŒ„å·²ç¶“å‡ºç¾çš„æ–°èæ¨™é¡Œï¼Œé¿å…é‡è¤‡
    seen_links = set()  # ğŸ”¹ è¨˜éŒ„å·²ç¶“å‡ºç¾çš„æ–°èé€£çµï¼Œé¿å…é‡è¤‡
    page = 1

    while len(news_results) < 35:
        try:
            googlenews.search(query)
            news = googlenews.result()

            print(f"ğŸ“Œ çˆ¬å–ç¬¬ {page} é æ–°èï¼Œç²å–åˆ° {len(news)} å‰‡")  # Debug è¨Šæ¯
            
            if not news:
                break

            for article in news:
                title = article.get('title', '')
                link = article.get('link', '')

                # ğŸ”¹ ä¿®æ­£é€£çµæ ¼å¼
                link = clean_link(link)

                # ğŸ”¹ é¿å…é‡è¤‡æ¨™é¡Œæˆ–é€£çµ
                if title in seen_titles or link in seen_links:
                    continue

                # ğŸ”¹ é¿å…ç„¡æ•ˆé€£çµï¼ˆYahoo éš±ç§é é¢ï¼‰
                if not link.startswith("http") or "consent.yahoo.com" in link:
                    continue

                seen_titles.add(title)  # è¨˜éŒ„æ¨™é¡Œï¼Œé¿å…é‡è¤‡
                seen_links.add(link)  # è¨˜éŒ„é€£çµï¼Œé¿å…é‡è¤‡
                news_results.append({
                    'title': title,
                    'media': article.get('media', ''),
                    'date': article.get('date', ''),
                    'link': link  # ğŸ”¹ ç¢ºä¿é€£çµæ ¼å¼æ­£ç¢º
                })

                # ğŸ”¹ å¦‚æœè¶…é 35 å‰‡ï¼Œè·³å‡ºè¿´åœˆ
                if len(news_results) >= 35:
                    break

            googlenews.get_page(page)
            page += 1
            time.sleep(3)  # ğŸ”¹ é¿å…éå¿«è«‹æ±‚è¢«å°é–

        except urllib.error.HTTPError as e:
            if e.code == 429:
                print("âš ï¸ Too Many Requests - ç­‰å¾… 10 ç§’å¾Œé‡è©¦...")
                time.sleep(10)  # å¦‚æœè¢«å°é–ï¼Œç­‰ 10 ç§’å¾Œé‡è©¦
                continue
            else:
                print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
                break

    print(f"âœ… æœ€çµ‚ç²å–åˆ° {len(news_results)} å‰‡æ–°è")  # Debug ç¢ºèª
    return news_results if news_results else []  # ç¢ºä¿æ°¸é å›å‚³åˆ—è¡¨
